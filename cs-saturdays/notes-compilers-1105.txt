Notes on compilers - 11/5 David Yang lecture

Compilers/interpreters 
	5 stages: 
	lexing, parsing, _______


	- optimization is the root of all evil 
	- 

	compiler theory- 
		combines CS fields including data strucures (parse trees, symbol tables) + algorithms (memory allocation, register allcation, stack mgmt, code optimization and minimization (tree-shaking, dead code elimination (of anything after reurn statement)))

	- compilers we use already - Angular ($parse service), React (JSX transformer- convert HTML to JS), babel.js, ESLint, SCSS, Uglify, ng-annotate 

	History of compilers
		- Ada Lovelace - invented first programming lang; check Stephen Wolfram on Ada Lovelace 
		- Alan Turing - Turing machine
		- Grace Hopper- created first compiler for language named A-0; later worked on COBOL 
		- John Backus - formalized idea of grammar vs language, B&F grammar 
		- Alfred Aho - "Dragon Book" of compiler theory 

	How compilers work 
	- source code --> frontend (parse and check input, semantic analysis) --> backend (optimize and generate code) --> lower-level code assembly, intermediate representation 
		- source code (main.js) --> 
		- frontend: 
			- lexical analysis (what is coming in?)
				- split into valid tokens: keywords, operators, semicolons, variable names. a 'token stream', each elmt is a valid token. 
				- parsing: based on relational structures of language (eg == is relation, = is assign, predicate, if-statement, else etc)
			- syntactic analysis (is it well-formed?)
			- semantic analysis (what does it mean?)
		--> parsed tree:
			how we get parsed tree depends on grammar of language. Grammar is all production rules - grammar defines production rules. 
				- given various productions of a grammar you can create stms in a L 
				- given statements in a language, can you parse it back into a tree of the production rules? --> this is the parsing process. 

			production rule: 
			E --> E + E (given an E on left, we could produce E+E.) 
			E --> E - E
			E --> E x E
			E --> -E
			E --> (E) 
			E --> num

			- Left-hand side produces right-hand side
			- LHS of production must be single symbol
				- nonterminals (single symbol on LHS) - doesn't terminate the parsing. Which means that it is a fn in set of recursive production rules. 
				- a language can have one or many nonterminals.
				- like the new state of a DFA 

			- RHS: a string with 0 or more symbols. Symbols that are not nonterminals are terminals. Here: +, -, num, (, ), x 

			- parsing is this whole thing in reverse: compiler inverts operations. 

			- language (L) created by grammar (G) is the set of all terminal strings that are the yield of a parse tree for the grammar. Any valid tree that can be generated using production rules of G are valid in language of L. 
				- language of JS is defined by production rules of the grammar of JS
				- a valid JS program has a tree generated by the production rules of the grammar of JS

			- parse tree: 1 + 2 x 3 
				E --> E->(num) + E  ->(num) --> E ->(num) x E ->(num)
				- concatenate tree leaves from L to R 
				- parse tree for string(x) --> proof that x is in lang. 

				parse tree (shows how lang is generated) vs abstract syntax tree (shows how tokens fit w each other, all details suppressed.)

			- static vs dynamically typed languages: type theory - loose vs strict typing. types impact more the performance of the code. compiler understands how code can be optimized. in dynamically typed lang space can be allocated more efficiently based on type assignments. in JS with loose typing this can be hard to do and it will slow things down. 

			- any language that is Turing complete can be used to author a compiler. 
						(lowest level) 
						- CPU (binary) 
						- assembly language (will have mov instructions etc to move a to b) 
						- C 
						--> branching out from here (and a language can recursively self-compile) 

		- parsing approaches: 
			- top-down
				- recursive descent parsing: use recursion to descend into the parse tree. 
					- manual parser creation
					- usually do it w a grammar defined as LL(1) 
						LL = left -> right, left-most derivation
						1 = look 1 token ahead in the token stream
					- caveats: 
						- prodxn rules have to be defined in a certain way 
						- no left recursions: E => E + E (these rules can't be parsed with recursive descent; recursion will never end). 
							Right recursion is fine.  
						- have to be able to determine which production rule based on next token (or, in some languages, look-ahead k tokens). 
							- If path is not clear, then there are predictive parsers that try to make guesses and then come back --> for instance, => in Javascript. 

					E --> TA
					A --> + TA
					A --> Epsilon (nothing) (this is like a base case; bc A can eventually become an epsilon recursion terminates)
					T --> FB
					B --> * FB
					B --> Epsilon
					F --> (E)
					F --> n

					parse_E(): 
						t1 = parse_T();
						t2 = parse_A();
						return makeTree("E", t1, t2) //call fns representing production rules. 

					//A has 2 production rules: if look-ahead by 1 sees a plus sign, it will go to first rule. if not, it goes to epsilon (nothing) 

					parse_A(): 
						tok = getlex();
						if (tok == '+') 
						then -- A -> + TA
							t1 = parse_T()
							t2 = parse_A()
							return makeTree('A', '+', t1, t2)
						else
							-- A -> epsilon
							return makeTree('A')
						etc. 

					1 * 3 + (4 * 7) 
					tokens: 1, +, 3, +, (, 4, *, 7, )
					E -> T -> (immediately calls) -> F -> n (1, which is returned up to T) 
						branching out from T: -> B (multiply) -> F (sees 3 as n, converts it to num, spits it back up) -> now B has consumed * and 3 and calls itself -> B looks ahead in token stream, sees + but has no rule for it, so it converts it to epsilon (dead case) 
					--> branching from E: calls A, A sees + sign and calls T --> T calls F, F sees ( and consumes it, then F calls E (which is 4+7, and then lookahead and recursive fns go through it: E --> T which goes to F which gets 4 and returns num; T comes back up, T goes to B, which consumes *, and B goes to F and F -> 7, spits back up, B then goes to epsilon and ends. E calls A, A doesn't see + so it goes to spilon. Then E goes back up to F and F consumes ). 

			- bottom-up 
				- backtracking (recursion) 
				- shift-reduce parsing 
					- how parser generators usually work: give it grammar and it will generate tables of parser for you. Things in language are shifted onto array and when it sees what to do it will reduce it back onto the stack. 
					- unfortunately the tables generated by parser generators are quite unreadable. 
		- Grammar for math exprsns: has rules laying out what valid tokens are and then rules for precedence (+ and - vs * and /) 
			cf. grammar for JS online

		- whitespace: if a lang ignores whitespace, when tokenizing the white spaces don't show up in the token. Parser will use indent to see whether it is in fn body vs next statement. 
		- Parser generators: 
			- Lex/Yacc
			- Bison etc 
		- uglify.js - efficiency/performance over bandwidth, and making it harder to mess up.
		- LLVM (low-level virtual machine) - for standardizing langs - standardization in backend. cf. llvm compiler infrastructure. 
		- webassembly- the only credible threat to JS as a language - gives browser a very low-level CPU language and you use other langs inside browser to write code. Est. 2018-2019?

	Key takeaways
	- program itself is a piece of data and it moves data around
	- compilers have multiple stages: 2 main tasks are lexing and parsing, first
	- programming langs are defined by grammars
		- backus-naur form (BNF)
	- two primary ways to parse L into G- top-down and bottom-up 
	- top-down parsing can be done with recursive descent if language is LL(1) (L to R, L recursive, 1 token lookahead) -- most languages can be refactored into a format that is LL(1)-ish. 
	- books to check out: 
		- create your own freaking awesome programming language
		- language implementation patterns
		- nanopass framework for compiler implementation (paper)
		- tiny javascript compiler 


